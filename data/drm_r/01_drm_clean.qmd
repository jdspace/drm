---
title: "01_drm_clean"
author: "Jeremy Faulk"
format: html
editor: visual
---

## Setup

```{r setup, include=FALSE}
# ğŸ“¦ Load helpful libraries
library(here)
library(readxl)
library(janitor)
library(tidyverse)
library(scales)
library(knitr)
library(purrr)
library(naniar)

# ğŸ“Œ Declare project root for 'here' (only needed once per session)
here::i_am("data/drm_r/01_drm_clean.qmd")
```

##ğŸ“¥ Load Combined DRM Dataset

```{r}
# ğŸ” Define and read the combined DRM data file
path <- here("data", "drm_merged", "drm_qualtrics_quantitative.csv")
if (!file.exists(path)) stop("File not found: ", path)
# First row is column label
df <- read_csv(path)

# ğŸ‘€ Preview structure
df %>%
  slice_head(n = 10)
```

##ğŸ§¼ Clean and Standardize

```{r}
## ğŸ§· Load and Attach Human-Readable Variable Labels (Pre-Cleaning)

# ğŸ“¤ Load human-readable variable labels
question_list <- read_csv(here("data", "drm_merged", "drm_question_list.csv"))

# ğŸ§¤ Clean and match question list
question_list <- question_list %>%
  rename(
    variable_raw = 1,  # original names from Qualtrics
    label = 2,
    level = 3
  ) %>%
  mutate(
    variable = make_clean_names(variable_raw),
    label = str_trim(label),
    level = as.character(level)
  )

# ğŸ—‘ï¸ Remove metadata/question label rows
df <- df[-c(2, 75, 81, 85, 90, 92, 95), ]

# ğŸ§½ Standardize column names BEFORE defining original_names
df <- clean_names(df)

# ğŸ’¾ Now capture the cleaned names (after cleaning)
original_names <- names(df)

# ğŸš® Remove junk rows that start with '{"ImportId":' in the SONA column
df <- df %>%
  filter(!str_detect(sona, "^\\{\"ImportId\":"))

# ğŸ·ï¸ Filter question_list for vars in df and build lookup
label_lookup <- question_list %>%
  filter(variable %in% original_names) %>%
  select(variable, label, level)

# ğŸ§¾ Merge with your working df's variable names
variable_metadata <- tibble(variable = original_names) %>%
  left_join(label_lookup, by = "variable")

# âœ… Confirm remaining rows
cat("Remaining rows after removing junk entries:", nrow(df), "\n")

# ğŸ§¼ Normalize SONA values (lowercase + trim whitespace)
df <- df %>%
  mutate(sona = str_trim(tolower(sona)))

# ğŸ§¾ SONA IDs to exclude
drop_ids <- tolower(c(
  "Jps395@cornell.edu", "Njw75@cornell.edu", "92419", "87832", 
  "bjl99@cornell.edu", "74467", "Kl547@cornell.edu", "86920", 
  "km656@cornell.edu", "71665", "93025"
))

# ğŸš« Filter out excluded participants
df <- df %>%
  filter(!(sona %in% drop_ids))

cat("Remaining rows after exclusion:", nrow(df), "\n")

# ğŸ§½ Remove duplicate suffixes (keep only .x columns if .y exists)
df <- df %>%
  select(-matches("\\.y$")) %>%
  rename_with(~ str_remove(., "\\.x$"), matches("\\.x$"))

# ğŸ” Convert character-like columns to numeric, EXCLUDING `sona`
df <- df %>%
  mutate(across(
    .cols = intersect(names(select(., where(is.character))), names(df)[names(df) != "sona"]),
    .fns = ~ suppressWarnings(as.numeric(.))
  ))

# ğŸ—‘ï¸ Identify all-NA columns
na_only_cols <- df %>%
  select(where(~ all(is.na(.)))) %>%
  names()

cat("ğŸ”» Dropping", length(na_only_cols), "columns that are all NA\n")
print(na_only_cols)

# ğŸ§¾ Save audit log of dropped columns
na_log_path <- here::here("data", "drm_r", "outputs", paste0("dropped_na_columns_", Sys.Date(), ".csv"))
write_csv(tibble(dropped_column = na_only_cols), na_log_path)

cat("âœ… Dropped column names saved to:\n", na_log_path, "\n")

# ğŸ§¹ Drop those columns from the dataframe
df <- df %>%
  select(-all_of(na_only_cols))

# ğŸ” Find duplicated SONA IDs
duplicated_ids <- df %>%
  filter(sona %in% sona[duplicated(sona) | duplicated(sona, fromLast = TRUE)])

# ğŸ’¾ Save to output folder for auditing
write_csv(duplicated_ids, here("data", "drm_r", "outputs", "duplicated_ids.csv"))
```

## Identify Person-Level vs. Episode-Level Dataframes + Coalesce Rows

```{r}
# ğŸ“Š Coalesce rows: keep *non-NA values* where possible
df_merged <- df %>%
  group_by(sona) %>%
  summarise(across(everything(), ~ reduce(.x, coalesce)), .groups = "drop")

# ğŸ” Identify numeric columns from df_merged, EXCLUDING `sona` if it was coerced
numeric_cols <- df_merged %>%
  select(where(is.numeric)) %>%
  names() %>%
  setdiff("sona")

# ğŸ“‹ Preview
head(numeric_cols)

# ğŸ§µ Split into person- and episode-level variable names (after df_merged is defined)
person_vars <- variable_metadata %>%
  filter(level == "1") %>%
  pull(variable)

episode_vars <- variable_metadata %>%
  filter(level == "2") %>%
  pull(variable)

# âœ‚ï¸ Split into two dataframes
df_person <- df_merged %>% select(any_of(person_vars))
df_episode <- df_merged %>% select(sona, any_of(episode_vars))

# âœ… Confirm splits
cat("Person-level vars:", length(person_vars), "\n")
cat("Episode-level vars:", length(episode_vars), "\n")

table(variable_metadata$level, useNA = "ifany")

# ğŸ” Identify numeric columns containing 99s
columns_with_99 <- df %>%
  select(where(is.numeric)) %>%
  select(where(~ any(. == 99, na.rm = TRUE))) %>%
  names()

# ğŸ“Š Count how many 99s are in each column
count_99s <- sapply(df[columns_with_99], function(x) sum(x == 99, na.rm = TRUE))
cat("ğŸ”¢ Number of 99s to convert to NA:\n")
print(count_99s)

# ğŸ”„ Replace 99s with NA in those columns
df[columns_with_99] <- lapply(df[columns_with_99], function(x) replace(x, x == 99, NA))

# ğŸ“ Identify duplicated sona IDs (before merging)
duplicated_ids_list <- df %>%
  count(sona) %>%
  filter(n > 1) %>%
  pull(sona)

# ğŸ“„ Create a tibble of all rows with those IDs
duplicated_ids <- df %>%
  filter(sona %in% duplicated_ids_list) %>%
  arrange(sona)

# ğŸ’¾ Export to CSV for manual inspection
output_path <- here::here("data", "drm_r", "outputs", "duplicated_ids.csv")
write_csv(duplicated_ids, output_path)

# ğŸ‘ï¸ Done! View it in RStudio
View(duplicated_ids)

# ğŸ§¾ Check number of unique vs original SONA rows
cat("Original rows:", nrow(df), "\n")
cat("After merge:", nrow(df_merged), "\n")

# ğŸªŸ View a few cases to verify
df_merged %>% filter(sona %in% c("78559", "72349")) %>% View()

# âœ… Confirm remaining rows
cat("Remaining rows after coalescing duplicate sona entries:", nrow(df), "\n")
```

## ğŸš« Drop Participants Without Episode-Level Data

```{r}

# ğŸ§¼ Drop rows where `sona` matches any in `drop_ids`
df <- df %>% 
  filter(!(sona %in% drop_ids))

# âœ… Confirm how many rows remain
cat("Remaining participants after exclusion:", nrow(df), "\n")
```

## ğŸš« Drop Columns Without Useful Data
```{r}
# ğŸ§¹ Identify columns to drop by pattern (prefix or exact match)
cols_to_drop <- grep(
  "^start_date|^end_date|^status|^progress|^ip_address|^q_recaptcha_score|^finished|^distribution_channel|^user_language|^no_consent_exit|^recorded_date|^response_id",
  names(df_merged),
  value = TRUE
)

# ğŸ‘€ Preview the columns that will be removed
cat("Columns to be dropped:\n")
print(cols_to_drop)

# ğŸ§½ Drop the columns from df_merged and assign to df
df <- df_merged %>%
  select(-all_of(cols_to_drop))

cat("Final number of rows:", nrow(df), "\n")
head(df)

```

##âœ… ğŸ” Flag Missing SONA IDs

```{r}
df <- df %>%
  mutate(sona_status = case_when(
    is.na(sona) ~ "missing",
    TRUE ~ "present"
  ))

# ğŸ”¢ Count how many are missing
table(df$sona_status)
```

## Create Human-Readable Labels for Graphs and Tables

```{r}
# ğŸ“› Return human-readable label if available, else fallback to variable name
labelize <- function(var_name, metadata = variable_metadata) {
  label <- metadata %>%
    filter(variable == var_name) %>%
    pull(label)
  
  if (length(label) == 0 || is.na(label)) {
    return(var_name)  # fallback to variable name
  } else {
    return(label)
  }
}

```

## ğŸ“Š Export Summary Statistics

```{r}
# ğŸ“‹ Create a summary table
summary_df <- as.data.frame(summary(df))

# ğŸ’¾ Define output file path
output_file <- here("data", "drm_r", "outputs", paste0("summary_", Sys.Date(), ".csv"))

# ğŸ“¤ Save to outputs folder
write_csv(summary_df, output_file)
```

## Manual Instructions to Commit to GitHub (i.e., not auto)

```{text}
# ğŸš€ Manually run this chunk when you're ready to commit

# Set your commit message manually here
message <- "Quick update via Quarto chunk"

# Commit and push to GitHub
system("git add .")
system(paste('git commit -m', shQuote(message)))
system("git push")

---
### ğŸ§  Why `eval=FALSE`?

# - So the commit isnâ€™t triggered **automatically** every time you render.
# - Youâ€™ll run this chunk manually (e.g., click the green â–¶ï¸ in the corner), **when** you want to push to GitHub.

```
--------------------

## ğŸš« .gitignore Reference

```{text}
# RStudio project files
.Rproj.user/
.Rhistory
.RData
.DS_Store

# Quarto/knitr cache and output
*_cache/
*_files/

# Rendered output files
outputs/
*.html
*.pdf
*.docx

# Raw data
data/*.csv
data/*.xlsx
```

## ğŸš§ Identify Numeric Variables

```{r}
# ğŸ” Re-identify numeric columns from final df
numeric_cols <- df %>%
  select(where(is.numeric)) %>%
  names()

# ğŸ§¾ Print preview of available numeric columns
cat("Preview of numeric columns in final df:\n")
print(head(numeric_cols))

# ğŸ‘€ Check first few rows of numeric data
df %>%
  select(all_of(head(numeric_cols, 5))) %>%
  slice_head(n = 10)


# ğŸ” Try to convert character columns to numeric where possible
df <- df %>%
  mutate(across(where(is.character), ~ suppressWarnings(as.numeric(.x)), .names = "num_{.col}"))

# ğŸ” Now select the newly created numeric columns
numeric_cols <- df %>%
  select(starts_with("num_")) %>%
  names()

# ğŸ§¾ Confirm that it worked
head(numeric_cols)
```

## ğŸš¨ Flag Potential Outliers (Â±3 SD)

```{r}
# ğŸ” Re-identify numeric columns from df
numeric_cols <- df %>%
  select(where(is.numeric)) %>%
  names()

# ğŸ”¢ Show how many numeric columns
cat("Number of numeric columns being checked for outliers:", length(numeric_cols), "\n")

# âš ï¸ Flag outliers using Â±3 SD rule
outlier_df <- df %>%
  select(all_of(numeric_cols)) %>%
  pivot_longer(cols = everything(), names_to = "variable", values_to = "value") %>%
  group_by(variable) %>%
  mutate(
    mean_val = mean(value, na.rm = TRUE),
    sd_val   = sd(value, na.rm = TRUE),
    is_outlier = abs(value - mean_val) > 3 * sd_val
  ) %>%
  filter(is_outlier)

# ğŸ§¾ Show results
cat("Number of outliers flagged:", nrow(outlier_df), "\n")
head(outlier_df)

```

## âœ… Summary of missingness

```{r}
# ğŸ“Š Summary of missing values per variable
missing_summary_person <- df_person %>%
  summarise(across(everything(), ~ sum(is.na(.)))) %>%
  pivot_longer(everything(), names_to = "variable", values_to = "n_missing") %>%
  arrange(desc(n_missing))

missing_summary_episode <- df_episode %>%
  summarise(across(everything(), ~ sum(is.na(.)))) %>%
  pivot_longer(everything(), names_to = "variable", values_to = "n_missing") %>%
  arrange(desc(n_missing))
```

## ğŸ§¾ Attach Labels to Missingness Tables

```{r}
# Add human-readable labels
missing_summary_person <- missing_summary_person %>%
  left_join(variable_metadata, by = "variable") %>%
  arrange(desc(n_missing))

missing_summary_episode <- missing_summary_episode %>%
  left_join(variable_metadata, by = "variable") %>%
  arrange(desc(n_missing))
```

## ğŸ’¾ Save Audit Files

```{r}
write_csv(missing_summary_person, here("data", "drm_r", "outputs", "missing_summary_person.csv"))
write_csv(missing_summary_episode, here("data", "drm_r", "outputs", "missing_summary_episode.csv"))
```

## ğŸ” Identify High-Missingness Variables
```{r}
# ğŸ§® Set threshold to flag high missingness
threshold <- 0.5

# ğŸš© Person-level variables
high_missing_person <- missing_summary_person %>%
  mutate(missing_prop = n_missing / nrow(df_person)) %>%
  filter(missing_prop > threshold)

# ğŸš© Episode-level variables
high_missing_episode <- missing_summary_episode %>%
  mutate(missing_prop = n_missing / nrow(df_episode)) %>%
  filter(missing_prop > threshold)

# ğŸ’¾ Save to outputs
write_csv(high_missing_person, here("data", "drm_r", "outputs", "high_missing_person.csv"))
write_csv(high_missing_episode, here("data", "drm_r", "outputs", "high_missing_episode.csv"))
```

## ğŸ”¬ Optional: Visualize Missingness

```{r}
library(naniar)

# ğŸ” Visual patterns of missingness
gg_miss_upset(df_person)
gg_miss_upset(df_episode)

# Or a heatmap-style overview
vis_miss(df_person, sort_miss = TRUE) + ggtitle("Missing Map â€” Person-Level")
vis_miss(df_episode, sort_miss = TRUE) + ggtitle("Missing Map â€” Episode-Level")
```

## ğŸ”„ Recode NAs to 0 for Multi-Selection Columns (Episode-Level)

```{r}
# ğŸ§© Some columns use NA to mean "not selected" â€” convert to 0
df_episode <- df_episode %>%
  mutate(across(matches("^c[0-9]+option[0-9]+$"), ~ replace_na(., 0)))  # adjust regex as needed
```

## ğŸš© Flag Participants with Excessive Missingness in a Key Variable

```{r}
# ğŸ“Œ Placeholder for composite variable missingness
# To activate once emotion_regulation_score is created
# df_episode %>%
#   group_by(sona) %>%
#   summarise(missing_rate = mean(is.na(emotion_regulation_score))) %>%
#   filter(missing_rate > 0.5)
```

## ğŸ“Š Summarize With NA-Resilient Functions

```{r}
# ğŸ§  Use summary stats that handle NA automatically
summary_stats <- df %>%
  summarise(across(where(is.numeric), list(
    mean = ~mean(., na.rm = TRUE),
    sd   = ~sd(., na.rm = TRUE),
    n    = ~sum(!is.na(.))
  ), .names = "{.col}_{.fn}"))

# ğŸ’¾ Export if needed
write_csv(summary_stats, here("data", "drm_r", "outputs", "summary_stats_resilient.csv"))
```

## Tag Columns That Represent Non-Selections

```{text}

# Recode NAs to 0 if NA means "not selected"
df_episode <- df_episode %>%
  mutate(across(matches("^c[0-9]+option[0-9]+$"), ~ replace_na(., 0)))  # adjust regex as needed

```

## ğŸš© Flag Cases with Partial Completion

```{text}
# ğŸ›‘ Mark participants with >30% missing on "q" items
df <- df %>%
  mutate(partially_complete = rowMeans(is.na(across(starts_with("q")))) > 0.3)

# ğŸ“Š Count of partial completions
table(df$partially_complete)
```

ğŸ“ Document Assumptions About Missingness

```{markdown}
### Assumptions About Missing Data

- Missing values arise from skipped items, forgotten entries, or non-applicability.
- Multi-select NAs (e.g., `c01option1`) were recoded to `0` to reflect "not selected."
- Summary statistics are NA-resilient (`na.rm = TRUE`).
- No listwise deletion or imputation was used at this stage of analysis.
- Variables exceeding 50% missingness were logged separately.
```

ğŸ—ƒï¸ Create Final Missingness Audit Table

```{r}
missingness_df <- df_merged %>%
  summarise(across(everything(), ~ sum(is.na(.)))) %>%
  pivot_longer(everything(), names_to = "variable", values_to = "n_missing") %>%
  arrange(desc(n_missing)) %>%
  left_join(variable_metadata, by = "variable")

# ğŸ’¾ Save for recordkeeping
write_csv(missingness_df, here("data", "drm_r", "outputs", "missingness_df.csv"))
```
