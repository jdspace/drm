---
title: "01_drm_clean"
author: "Jeremy Faulk"
format: html
editor: visual
---

## Setup

```{r setup, include=FALSE}
# 📦 Load helpful libraries
library(here)
library(readxl)
library(janitor)
library(tidyverse)
library(scales)
library(knitr)
library(purrr)

# 📌 Declare project root for 'here' (only needed once per session)
here::i_am("data/drm_r/01_drm_clean.qmd")
```

##📥 Load Combined DRM Dataset

```{r}
# 🔍 Define and read the combined DRM data file
path <- here("data", "drm_merged", "drm_qualtrics_quantitative.csv")
if (!file.exists(path)) stop("File not found: ", path)
# First row is column label
df <- read_csv(path)

# 👀 Preview structure
df %>%
  slice_head(n = 10)
```

##🧼 Clean and Standardize

```{r}
# 🗑️ Remove metadata/question label rows
df <- df[-c(2, 75, 81, 85, 90, 92, 95), ]

# 🧽 Standardize column names
df <- clean_names(df)

# 🚮 Remove junk rows that start with '{\"ImportId\":' in the SONA column
df <- df %>%
  filter(!str_detect(sona, "^\\{\"ImportId\":"))

# ✅ Confirm remaining rows
cat("Remaining rows after removing junk entries:", nrow(df), "\n")

# 🧼 Normalize SONA values (lowercase + trim whitespace)
df <- df %>%
  mutate(sona = str_trim(tolower(sona)))

# 🧾 SONA IDs to exclude (also lowercase to match)
drop_ids <- tolower(c(
  "Jps395@cornell.edu", "Njw75@cornell.edu", "92419", "87832", 
  "bjl99@cornell.edu", "74467", "Kl547@cornell.edu", "86920", 
  "km656@cornell.edu", "71665", "93025"
))

# 🚫 Filter out excluded participants
df <- df %>%
  filter(!(sona %in% drop_ids))

# ✅ Check result
cat("Remaining rows after exclusion:", nrow(df), "\n")

# 🧽 Remove duplicate suffixes (keep only .x columns if .y exists)
df <- df %>%
  select(-matches("\\.y$")) %>%  # Drop columns ending in .y
  rename_with(~ str_remove(., "\\.x$"), matches("\\.x$"))  # Remove .x from column names

# 🔁 Convert character-like columns to numeric, EXCLUDING `sona`
df <- df %>%
  mutate(across(
    .cols = intersect(names(select(., where(is.character))), names(df)[names(df) != "sona"]),
    .fns = ~ suppressWarnings(as.numeric(.))
  ))

# 🧾 Find duplicated SONA IDs
duplicated_ids <- df %>%
  filter(sona %in% sona[duplicated(sona) | duplicated(sona, fromLast = TRUE)])

# 💾 Save to output folder
write_csv(duplicated_ids, here("data", "drm_r", "outputs", "duplicated_ids.csv"))

# 📊 Coalesce rows: keep *non-NA values* where possible
df_merged <- df %>%
  group_by(sona) %>%
  summarise(across(everything(), ~ reduce(.x, coalesce)), .groups = "drop")

debug_df <- df %>%
  filter(sona %in% duplicated_ids_list) %>%
  arrange(sona)
View(debug_df)

# 🔍 Identify numeric columns from df_merged, EXCLUDING `sona` if it was coerced
numeric_cols <- df_merged %>%
  select(where(is.numeric)) %>%
  names() %>%
  setdiff("sona")

# 📋 Preview
head(numeric_cols)

# 🔍 Identify numeric columns containing 99s
columns_with_99 <- df %>%
  select(where(is.numeric)) %>%
  select(where(~ any(. == 99, na.rm = TRUE))) %>%
  names()

# 📊 Count how many 99s are in each column
count_99s <- sapply(df[columns_with_99], function(x) sum(x == 99, na.rm = TRUE))
cat("🔢 Number of 99s to convert to NA:\n")
print(count_99s)

# 🔄 Replace 99s with NA in those columns
df[columns_with_99] <- lapply(df[columns_with_99], function(x) replace(x, x == 99, NA))

# 📍 Identify duplicated sona IDs (before merging)
duplicated_ids_list <- df %>%
  count(sona) %>%
  filter(n > 1) %>%
  pull(sona)

# 📄 Create a tibble of all rows with those IDs
duplicated_ids <- df %>%
  filter(sona %in% duplicated_ids_list) %>%
  arrange(sona)

# 💾 Export to CSV for manual inspection
output_path <- here::here("data", "drm_r", "outputs", "duplicated_ids.csv")
write_csv(duplicated_ids, output_path)

# 👁️ Done! View it in RStudio
View(duplicated_ids)

# 🧾 Check number of unique vs original SONA rows
cat("Original rows:", nrow(df), "\n")
cat("After merge:", nrow(df_merged), "\n")

# 🪟 View a few cases to verify
df_merged %>% filter(sona %in% c("78559", "72349")) %>% View()

# ✅ Confirm remaining rows
cat("Remaining rows after coalescing duplicate sona entries:", nrow(df), "\n")
```

## 🚫 Drop Participants Without Episode-Level Data

```{r}

# 🧼 Drop rows where `sona` matches any in `drop_ids`
df <- df %>% 
  filter(!(sona %in% drop_ids))

# ✅ Confirm how many rows remain
cat("Remaining participants after exclusion:", nrow(df), "\n")
```

## 🚫 Drop Columns Without Useful Data
```{r}
# 🧹 Identify columns to drop by pattern (prefix or exact match)
cols_to_drop <- grep(
  "^start_date|^end_date|^status|^progress|^ip_address|^q_recaptcha_score|^finished|^distribution_channel|^user_language|^no_consent_exit|^recorded_date|^response_id",
  names(df_merged),
  value = TRUE
)

# 👀 Preview the columns that will be removed
cat("Columns to be dropped:\n")
print(cols_to_drop)

# 🧽 Drop the columns from df_merged and assign to df
df <- df_merged %>%
  select(-all_of(cols_to_drop))

cat("Final number of rows:", nrow(df), "\n")
head(df)

```

##✅ 🔍 Flag Missing SONA IDs

```{r}
df <- df %>%
  mutate(sona_status = case_when(
    is.na(sona) ~ "missing",
    TRUE ~ "present"
  ))

# 🔢 Count how many are missing
table(df$sona_status)
```

## 📊 Export Summary Statistics

```{r}
# 📋 Create a summary table
summary_df <- as.data.frame(summary(df))

# 💾 Define output file path
output_file <- here("data", "drm_r", "outputs", paste0("summary_", Sys.Date(), ".csv"))

# 📤 Save to outputs folder
write_csv(summary_df, output_file)
```

## Manual Instructions to Commit to GitHub (i.e., not auto)

```{text}
# 🚀 Manually run this chunk when you're ready to commit

# Set your commit message manually here
message <- "Quick update via Quarto chunk"

# Commit and push to GitHub
system("git add .")
system(paste('git commit -m', shQuote(message)))
system("git push")

---
### 🧠 Why `eval=FALSE`?

# - So the commit isn’t triggered **automatically** every time you render.
# - You’ll run this chunk manually (e.g., click the green ▶️ in the corner), **when** you want to push to GitHub.

```
--------------------

## 🚫 .gitignore Reference

```{text}
# RStudio project files
.Rproj.user/
.Rhistory
.RData
.DS_Store

# Quarto/knitr cache and output
*_cache/
*_files/

# Rendered output files
outputs/
*.html
*.pdf
*.docx

# Raw data
data/*.csv
data/*.xlsx
```

## 🚧 Identify Numeric Variables

```{r}
# 🔍 Re-identify numeric columns from final df
numeric_cols <- df %>%
  select(where(is.numeric)) %>%
  names()

# 🧾 Print preview of available numeric columns
cat("Preview of numeric columns in final df:\n")
print(head(numeric_cols))

# 👀 Check first few rows of numeric data
df %>%
  select(all_of(head(numeric_cols, 5))) %>%
  slice_head(n = 10)


# 🔁 Try to convert character columns to numeric where possible
df <- df %>%
  mutate(across(where(is.character), ~ suppressWarnings(as.numeric(.x)), .names = "num_{.col}"))

# 🔍 Now select the newly created numeric columns
numeric_cols <- df %>%
  select(starts_with("num_")) %>%
  names()

# 🧾 Confirm that it worked
head(numeric_cols)
```

## 🚨 Flag Potential Outliers (±3 SD)

```{r}
# 🔍 Re-identify numeric columns from df
numeric_cols <- df %>%
  select(where(is.numeric)) %>%
  names()

# 🔢 Show how many numeric columns
cat("Number of numeric columns being checked for outliers:", length(numeric_cols), "\n")

# ⚠️ Flag outliers using ±3 SD rule
outlier_df <- df %>%
  select(all_of(numeric_cols)) %>%
  pivot_longer(cols = everything(), names_to = "variable", values_to = "value") %>%
  group_by(variable) %>%
  mutate(
    mean_val = mean(value, na.rm = TRUE),
    sd_val   = sd(value, na.rm = TRUE),
    is_outlier = abs(value - mean_val) > 3 * sd_val
  ) %>%
  filter(is_outlier)

# 🧾 Show results
cat("Number of outliers flagged:", nrow(outlier_df), "\n")
head(outlier_df)

```

