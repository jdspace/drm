---
title: "01_drm_clean"
author: "Jeremy Faulk"
format: html
editor: visual
---

## Setup

```{r setup, include=FALSE}
# ğŸ“¦ Load helpful libraries
library(here)
library(readxl)
library(janitor)
library(tidyverse)
library(scales)
library(knitr)
library(purrr)

# ğŸ“Œ Declare project root for 'here' (only needed once per session)
here::i_am("data/drm_r/01_drm_clean.qmd")
```

##ğŸ“¥ Load Combined DRM Dataset

```{r}
# ğŸ” Define and read the combined DRM data file
path <- here("data", "drm_merged", "drm_qualtrics_quantitative.csv")
if (!file.exists(path)) stop("File not found: ", path)
# First row is column label
df <- read_csv(path)

# ğŸ‘€ Preview structure
df %>%
  slice_head(n = 10)
```

##ğŸ§¼ Clean and Standardize

```{r}
# ğŸ—‘ï¸ Remove metadata/question label rows
df <- df[-c(2, 75, 81, 85, 90, 92, 95), ]

# ğŸ§½ Standardize column names
df <- clean_names(df)

# ğŸš® Remove junk rows that start with '{\"ImportId\":' in the SONA column
df <- df %>%
  filter(!str_detect(sona, "^\\{\"ImportId\":"))

# âœ… Confirm remaining rows
cat("Remaining rows after removing junk entries:", nrow(df), "\n")

# ğŸ§¼ Normalize SONA values (lowercase + trim whitespace)
df <- df %>%
  mutate(sona = str_trim(tolower(sona)))

# ğŸ§¾ SONA IDs to exclude (also lowercase to match)
drop_ids <- tolower(c(
  "Jps395@cornell.edu", "Njw75@cornell.edu", "92419", "87832", 
  "bjl99@cornell.edu", "74467", "Kl547@cornell.edu", "86920", 
  "km656@cornell.edu", "71665", "93025"
))

# ğŸš« Filter out excluded participants
df <- df %>%
  filter(!(sona %in% drop_ids))

# âœ… Check result
cat("Remaining rows after exclusion:", nrow(df), "\n")

# ğŸ§½ Remove duplicate suffixes (keep only .x columns if .y exists)
df <- df %>%
  select(-matches("\\.y$")) %>%  # Drop columns ending in .y
  rename_with(~ str_remove(., "\\.x$"), matches("\\.x$"))  # Remove .x from column names

# ğŸ” Convert character-like columns to numeric, EXCLUDING `sona`
df <- df %>%
  mutate(across(
    .cols = intersect(names(select(., where(is.character))), names(df)[names(df) != "sona"]),
    .fns = ~ suppressWarnings(as.numeric(.))
  ))

# ğŸ§¾ Find duplicated SONA IDs
duplicated_ids <- df %>%
  filter(sona %in% sona[duplicated(sona) | duplicated(sona, fromLast = TRUE)])

# ğŸ’¾ Save to output folder
write_csv(duplicated_ids, here("data", "drm_r", "outputs", "duplicated_ids.csv"))
```


## Identify Person-Level vs. Episode-Level Dataframes


```{r}
# ğŸ“‹ Create a variable-level tagging template for manual review
variable_levels <- tibble(
  variable = names(df),
  level = ""  # Placeholder to manually enter 'person' or 'episode'
)

# ğŸ’¾ Save to outputs folder for tagging
write_csv(variable_levels, here("data", "drm_r", "outputs", "variable_levels_template.csv"))

# ğŸ‘ï¸ View in RStudio's Data window
View(variable_levels)

cat("âœ… Variable-level tagging template saved to:\n",
    here("data", "drm_r", "outputs", "variable_levels_template.csv"), "\n")
```


## Identify Person-Level vs. Episode-Level Dataframes


```{text}
person_vars <- c("sona", "age", "gender", "baseline_wellbeing", ...)  # replace with actual vars
df_person <- df_merged %>% select(any_of(person_vars))

df_episode <- df_merged %>% select(-any_of(person_vars))
```

```{r}
# ğŸ“Š Coalesce rows: keep *non-NA values* where possible
df_merged <- df %>%
  group_by(sona) %>%
  summarise(across(everything(), ~ reduce(.x, coalesce)), .groups = "drop")

debug_df <- df %>%
  filter(sona %in% duplicated_ids_list) %>%
  arrange(sona)
View(debug_df)

# ğŸ” Identify numeric columns from df_merged, EXCLUDING `sona` if it was coerced
numeric_cols <- df_merged %>%
  select(where(is.numeric)) %>%
  names() %>%
  setdiff("sona")

# ğŸ“‹ Preview
head(numeric_cols)

# ğŸ” Identify numeric columns containing 99s
columns_with_99 <- df %>%
  select(where(is.numeric)) %>%
  select(where(~ any(. == 99, na.rm = TRUE))) %>%
  names()

# ğŸ“Š Count how many 99s are in each column
count_99s <- sapply(df[columns_with_99], function(x) sum(x == 99, na.rm = TRUE))
cat("ğŸ”¢ Number of 99s to convert to NA:\n")
print(count_99s)

# ğŸ”„ Replace 99s with NA in those columns
df[columns_with_99] <- lapply(df[columns_with_99], function(x) replace(x, x == 99, NA))

# ğŸ“ Identify duplicated sona IDs (before merging)
duplicated_ids_list <- df %>%
  count(sona) %>%
  filter(n > 1) %>%
  pull(sona)

# ğŸ“„ Create a tibble of all rows with those IDs
duplicated_ids <- df %>%
  filter(sona %in% duplicated_ids_list) %>%
  arrange(sona)

# ğŸ’¾ Export to CSV for manual inspection
output_path <- here::here("data", "drm_r", "outputs", "duplicated_ids.csv")
write_csv(duplicated_ids, output_path)

# ğŸ‘ï¸ Done! View it in RStudio
View(duplicated_ids)

# ğŸ§¾ Check number of unique vs original SONA rows
cat("Original rows:", nrow(df), "\n")
cat("After merge:", nrow(df_merged), "\n")

# ğŸªŸ View a few cases to verify
df_merged %>% filter(sona %in% c("78559", "72349")) %>% View()

# âœ… Confirm remaining rows
cat("Remaining rows after coalescing duplicate sona entries:", nrow(df), "\n")
```

## ğŸš« Drop Participants Without Episode-Level Data

```{r}

# ğŸ§¼ Drop rows where `sona` matches any in `drop_ids`
df <- df %>% 
  filter(!(sona %in% drop_ids))

# âœ… Confirm how many rows remain
cat("Remaining participants after exclusion:", nrow(df), "\n")
```

## ğŸš« Drop Columns Without Useful Data
```{r}
# ğŸ§¹ Identify columns to drop by pattern (prefix or exact match)
cols_to_drop <- grep(
  "^start_date|^end_date|^status|^progress|^ip_address|^q_recaptcha_score|^finished|^distribution_channel|^user_language|^no_consent_exit|^recorded_date|^response_id",
  names(df_merged),
  value = TRUE
)

# ğŸ‘€ Preview the columns that will be removed
cat("Columns to be dropped:\n")
print(cols_to_drop)

# ğŸ§½ Drop the columns from df_merged and assign to df
df <- df_merged %>%
  select(-all_of(cols_to_drop))

cat("Final number of rows:", nrow(df), "\n")
head(df)

```

##âœ… ğŸ” Flag Missing SONA IDs

```{r}
df <- df %>%
  mutate(sona_status = case_when(
    is.na(sona) ~ "missing",
    TRUE ~ "present"
  ))

# ğŸ”¢ Count how many are missing
table(df$sona_status)
```

## ğŸ“Š Export Summary Statistics

```{r}
# ğŸ“‹ Create a summary table
summary_df <- as.data.frame(summary(df))

# ğŸ’¾ Define output file path
output_file <- here("data", "drm_r", "outputs", paste0("summary_", Sys.Date(), ".csv"))

# ğŸ“¤ Save to outputs folder
write_csv(summary_df, output_file)
```

## Manual Instructions to Commit to GitHub (i.e., not auto)

```{text}
# ğŸš€ Manually run this chunk when you're ready to commit

# Set your commit message manually here
message <- "Quick update via Quarto chunk"

# Commit and push to GitHub
system("git add .")
system(paste('git commit -m', shQuote(message)))
system("git push")

---
### ğŸ§  Why `eval=FALSE`?

# - So the commit isnâ€™t triggered **automatically** every time you render.
# - Youâ€™ll run this chunk manually (e.g., click the green â–¶ï¸ in the corner), **when** you want to push to GitHub.

```
--------------------

## ğŸš« .gitignore Reference

```{text}
# RStudio project files
.Rproj.user/
.Rhistory
.RData
.DS_Store

# Quarto/knitr cache and output
*_cache/
*_files/

# Rendered output files
outputs/
*.html
*.pdf
*.docx

# Raw data
data/*.csv
data/*.xlsx
```

## ğŸš§ Identify Numeric Variables

```{r}
# ğŸ” Re-identify numeric columns from final df
numeric_cols <- df %>%
  select(where(is.numeric)) %>%
  names()

# ğŸ§¾ Print preview of available numeric columns
cat("Preview of numeric columns in final df:\n")
print(head(numeric_cols))

# ğŸ‘€ Check first few rows of numeric data
df %>%
  select(all_of(head(numeric_cols, 5))) %>%
  slice_head(n = 10)


# ğŸ” Try to convert character columns to numeric where possible
df <- df %>%
  mutate(across(where(is.character), ~ suppressWarnings(as.numeric(.x)), .names = "num_{.col}"))

# ğŸ” Now select the newly created numeric columns
numeric_cols <- df %>%
  select(starts_with("num_")) %>%
  names()

# ğŸ§¾ Confirm that it worked
head(numeric_cols)
```

## ğŸš¨ Flag Potential Outliers (Â±3 SD)

```{r}
# ğŸ” Re-identify numeric columns from df
numeric_cols <- df %>%
  select(where(is.numeric)) %>%
  names()

# ğŸ”¢ Show how many numeric columns
cat("Number of numeric columns being checked for outliers:", length(numeric_cols), "\n")

# âš ï¸ Flag outliers using Â±3 SD rule
outlier_df <- df %>%
  select(all_of(numeric_cols)) %>%
  pivot_longer(cols = everything(), names_to = "variable", values_to = "value") %>%
  group_by(variable) %>%
  mutate(
    mean_val = mean(value, na.rm = TRUE),
    sd_val   = sd(value, na.rm = TRUE),
    is_outlier = abs(value - mean_val) > 3 * sd_val
  ) %>%
  filter(is_outlier)

# ğŸ§¾ Show results
cat("Number of outliers flagged:", nrow(outlier_df), "\n")
head(outlier_df)

```

# NOTE: CONEVERT BELOW TO r INSTEAD OF TEXT

## Audio Missingness Patterns

```{r}
missing_summary <- df_merged %>%
  summarise(across(everything(), ~ sum(is.na(.)))) %>%
  pivot_longer(cols = everything(), names_to = "variable", values_to = "missing_n") %>%
  arrange(desc(missing_n))
```

## Tag Columns That Represent Non-Selections

```{text}

# Recode NAs to 0 if NA means "not selected"
df_episode <- df_episode %>%
  mutate(across(matches("^c[0-9]+option[0-9]+$"), ~ replace_na(., 0)))  # adjust regex as needed

```

## Set a Threshold to Flag â€œToo Much Missingâ€

```{text}

df_episode %>%
  group_by(sona) %>%
  summarise(missing_rate = mean(is.na(emotion_regulation_score))) %>%
  filter(missing_rate > 0.5)
```

## Don't Impute - Summarize With NA-Resilient Functions

```{text}
df %>%
  summarise(across(where(is.numeric), list(mean = ~mean(., na.rm = TRUE),
                                           sd = ~sd(., na.rm = TRUE),
                                           n = ~sum(!is.na(.)))))
```


## Flag Cases with Partial Completion

```{text}
df <- df %>%
  mutate(partially_complete = rowMeans(is.na(across(starts_with("q")))) > 0.3)
```

## Document Assumptions About Missingness

```{text}
In your analytic memo or script comments, briefly document:

- When missing = skipped vs. not applicable
- That you use NA-resilient summaries
- That no listwise deletion or imputation was used (unless added later)
```

## Create missingness_df for Review

```{text}
missingness_df <- df_merged %>%
  summarise(across(everything(), ~ sum(is.na(.)))) %>%
  pivot_longer(everything(), names_to = "variable", values_to = "n_missing") %>%
  arrange(desc(n_missing))
```

